{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from pydantic import SecretStr\n",
    "from pydantic_settings import BaseSettings\n",
    "\n",
    "class Settings(BaseSettings):\n",
    "    \"\"\"Settings for the demo app.\n",
    "\n",
    "    Reads from environment variables.\n",
    "    You can create the .env file from the .env_example file.\n",
    "\n",
    "    !!! SecretStr is a pydantic type that hides the value in logs.\n",
    "    If you want to use the real value, you should do:\n",
    "    SETTINGS.<variable>.get_secret_value()\n",
    "    \"\"\"\n",
    "\n",
    "    class Config:\n",
    "        env_file = \".env\"\n",
    "\n",
    "    openai_api_key: SecretStr\n",
    "    openai_model: str = \"gpt-4\"\n",
    "    temperature: float = 0\n",
    "\n",
    "    primary_doc_url: List[str] = ['https://medium.com/@andredp_33483/revolutionising-business-processes-integrating-slack-with-aws-using-serverless-architectures-1b5fb9cf1a0e']\n",
    "\n",
    "    secondary_docs_url: List[str] = [\n",
    "        'https://medium.com/serverless-transformation/building-a-robust-serverless-messaging-service-with-amazon-eventbridge-pipes-and-cdk-bf8250d10825',\n",
    "        'https://medium.com/serverless-transformation/enabling-the-optimal-serverless-platform-team-cdk-and-team-topologies-fe4d9299adc9',\n",
    "    ]\n",
    "    \n",
    "    supabase_url: str\n",
    "    supabase_service_key: SecretStr\n",
    "\n",
    "\n",
    "SETTINGS = Settings()  # type: ignore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.schema import Document\n",
    "from langchain_community.vectorstores import SupabaseVectorStore\n",
    "from supabase.client import Client, create_client\n",
    "\n",
    "def create_supabase_client() -> Client:\n",
    "    return create_client(\n",
    "        supabase_url=SETTINGS.supabase_url, \n",
    "        supabase_key=SETTINGS.supabase_service_key.get_secret_value()\n",
    "    )\n",
    "\n",
    "\n",
    "def create_gold_standard_vector_store() -> SupabaseVectorStore:\n",
    "    \"\"\"Create a vector store from a Supabase table.\"\"\"\n",
    "    return SupabaseVectorStore(\n",
    "        client=create_supabase_client(), \n",
    "        embedding=OpenAIEmbeddings(\n",
    "            openai_api_key=SETTINGS.openai_api_key.get_secret_value()\n",
    "            ), \n",
    "        table_name=\"documents\", \n",
    "        query_name=\"match_documents\",\n",
    "    )\n",
    "\n",
    "def create_draft_vector_store() -> SupabaseVectorStore:\n",
    "    \"\"\"Create a vector store from a Supabase table.\"\"\"\n",
    "    return SupabaseVectorStore(\n",
    "        client=create_supabase_client(), \n",
    "        embedding=OpenAIEmbeddings(\n",
    "            openai_api_key=SETTINGS.openai_api_key.get_secret_value()\n",
    "            ), \n",
    "        table_name=\"articles\", \n",
    "        query_name=\"match_articles\",\n",
    "    )\n",
    "\n",
    "\n",
    "def create_vector_store_from_documents(documents: list[Document], table_name: str) -> SupabaseVectorStore:\n",
    "    \"\"\"Create a vector store and populate it with a list of documents.\"\"\"\n",
    "    \n",
    "    embedding=OpenAIEmbeddings(\n",
    "            openai_api_key=SETTINGS.openai_api_key.get_secret_value()\n",
    "            )\n",
    "    client=create_supabase_client()\n",
    "\n",
    "    return SupabaseVectorStore.from_documents(\n",
    "        documents=documents,\n",
    "        embedding=embedding,\n",
    "        client=client,\n",
    "        table_name=table_name,\n",
    "        query_name=\"match_documents\",\n",
    "        # Lower chunk to prevent timeout\n",
    "        chunk_size=100,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import BaseRetriever\n",
    "from langchain_community.vectorstores import SupabaseVectorStore\n",
    "\n",
    "def get_retriever(vector_store: SupabaseVectorStore) -> BaseRetriever:\n",
    "    \"\"\"Basic retriever function to get a retriever from a vector store.\"\"\"\n",
    "    retriever = vector_store.as_retriever(search_kwargs={'k': 3}) # Default K is 4\n",
    "\n",
    "    return retriever\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['draft', 'gold_standard'] messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['draft', 'gold_standard'], template=\"You are Article GPT. The following pieces of context are considered to be the gold standard for future articles to aspire for. \\n{gold_standard}\\n\\nUse them to review and comment on the article you are about to read. You are to focus on the following aspects of the article:\\n- What is the tone of the article?\\n- What is the quality of the article?\\n- What is the level of detail of the article?\\n- What is the level of expertise of the article?\\n- What is the level of readability of the article?\\n- What is the level of relevance of the article?\\n\\nFor each of these aspects, you are to provide some feedback on the article in comparison to the gold standard articles. What works well? What doesn't work well? What could be improved?\\n----\\n{draft}\\n----\\n\"))]\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "system_template = \"\"\"You are Article GPT. The following pieces of context are considered to be the gold standard for future articles to aspire for. \n",
    "{gold_standard}\n",
    "\n",
    "Use them to review and comment on the article you are about to read. You are to focus on the following aspects of the article:\n",
    "- What is the tone of the article?\n",
    "- What is the quality of the article?\n",
    "- What is the level of detail of the article?\n",
    "- What is the level of expertise of the article?\n",
    "- What is the level of readability of the article?\n",
    "- What is the level of relevance of the article?\n",
    "\n",
    "For each of these aspects, you are to provide some feedback on the article in comparison to the gold standard articles. What works well? What doesn't work well? What could be improved?\n",
    "----\n",
    "{draft}\n",
    "----\n",
    "\"\"\"\n",
    "qa_prompt = ChatPromptTemplate.from_template(system_template)\n",
    "print(qa_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2130413304.py, line 23)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[17], line 23\u001b[0;36m\u001b[0m\n\u001b[0;31m    | qa_prompt\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=SETTINGS.openai_api_key.get_secret_value(),\n",
    "    model=SETTINGS.openai_model,\n",
    "    temperature=SETTINGS.temperature,\n",
    "    streaming=True,\n",
    ")\n",
    "\n",
    "gold_standard_vector_store = create_gold_standard_vector_store()\n",
    "gold_standard_retriever = get_retriever(gold_standard_vector_store)\n",
    "\n",
    "draft_vector_store = create_draft_vector_store()\n",
    "draft_retriever = get_retriever(draft_vector_store)\n",
    "\n",
    "rag_chain = (\n",
    "  {\n",
    "    \"gold_standard\": gold_standard_retriever,\n",
    "    \"draft\": draft_retriever,\n",
    "  },\n",
    "  | qa_prompt\n",
    "  | llm\n",
    "  | StrOutputParser()\n",
    ")\n",
    "rag_chain.invoke()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for StuffDocumentsChain\n__root__\n  document_variable_name context was not found in llm_chain input_variables: ['draft_article', 'gold_standard'] (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Use a faster model for condensing the question\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m qa \u001b[38;5;241m=\u001b[39m \u001b[43mConversationalRetrievalChain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_llm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretriever\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretriever1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcondense_question_llm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mChatOpenAI\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgpt-3.5-turbo\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopenai_api_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSETTINGS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopenai_api_key\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_secret_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcombine_docs_chain_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mqa_prompt\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/envs/article-gpt/lib/python3.11/site-packages/langchain/chains/conversational_retrieval/base.py:372\u001b[0m, in \u001b[0;36mConversationalRetrievalChain.from_llm\u001b[0;34m(cls, llm, retriever, condense_question_prompt, chain_type, verbose, condense_question_llm, combine_docs_chain_kwargs, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Convenience method to load chain from LLM and retriever.\u001b[39;00m\n\u001b[1;32m    349\u001b[0m \n\u001b[1;32m    350\u001b[0m \u001b[38;5;124;03mThis provides some logic to create the `question_generator` chain\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;124;03m        ConversationalRetrievalChain\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    371\u001b[0m combine_docs_chain_kwargs \u001b[38;5;241m=\u001b[39m combine_docs_chain_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[0;32m--> 372\u001b[0m doc_chain \u001b[38;5;241m=\u001b[39m \u001b[43mload_qa_chain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchain_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchain_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcombine_docs_chain_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m _llm \u001b[38;5;241m=\u001b[39m condense_question_llm \u001b[38;5;129;01mor\u001b[39;00m llm\n\u001b[1;32m    381\u001b[0m condense_question_chain \u001b[38;5;241m=\u001b[39m LLMChain(\n\u001b[1;32m    382\u001b[0m     llm\u001b[38;5;241m=\u001b[39m_llm,\n\u001b[1;32m    383\u001b[0m     prompt\u001b[38;5;241m=\u001b[39mcondense_question_prompt,\n\u001b[1;32m    384\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m    385\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m    386\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/envs/article-gpt/lib/python3.11/site-packages/langchain/chains/question_answering/__init__.py:250\u001b[0m, in \u001b[0;36mload_qa_chain\u001b[0;34m(llm, chain_type, verbose, callback_manager, **kwargs)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chain_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m loader_mapping:\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    247\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot unsupported chain type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchain_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    248\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloader_mapping\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    249\u001b[0m     )\n\u001b[0;32m--> 250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloader_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mchain_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/envs/article-gpt/lib/python3.11/site-packages/langchain/chains/question_answering/__init__.py:82\u001b[0m, in \u001b[0;36m_load_stuff_chain\u001b[0;34m(llm, prompt, document_variable_name, verbose, callback_manager, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m llm_chain \u001b[38;5;241m=\u001b[39m LLMChain(\n\u001b[1;32m     75\u001b[0m     llm\u001b[38;5;241m=\u001b[39mllm,\n\u001b[1;32m     76\u001b[0m     prompt\u001b[38;5;241m=\u001b[39m_prompt,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     79\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m     80\u001b[0m )\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# TODO: document prompt\u001b[39;00m\n\u001b[0;32m---> 82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mStuffDocumentsChain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm_chain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm_chain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocument_variable_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocument_variable_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/envs/article-gpt/lib/python3.11/site-packages/langchain_core/load/serializable.py:107\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/envs/article-gpt/lib/python3.11/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for StuffDocumentsChain\n__root__\n  document_variable_name context was not found in llm_chain input_variables: ['draft_article', 'gold_standard'] (type=value_error)"
     ]
    }
   ],
   "source": [
    "# Use a faster model for condensing the question\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm, \n",
    "    retriever=retriever1,\n",
    "    verbose=True,\n",
    "    memory=memory,\n",
    "    condense_question_llm = ChatOpenAI(\n",
    "        temperature=0, model='gpt-3.5-turbo', openai_api_key=SETTINGS.openai_api_key.get_secret_value()\n",
    "        ),\n",
    "    combine_docs_chain_kwargs={'prompt': qa_prompt}\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
